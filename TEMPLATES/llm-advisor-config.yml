# LLM Advisor Configuration Example for MegaLinter

# Enable AI-powered fix suggestions (requires LangChain dependencies)
LLM_ADVISOR_ENABLED: true

# LLM Provider Configuration
# Supported providers: openai, anthropic, google, ollama
LLM_PROVIDER: openai
LLM_MODEL_NAME: gpt-3.5-turbo
LLM_MAX_TOKENS: 1000
LLM_TEMPERATURE: 0.1

# Provider-specific API keys (set as environment variables)
# OPENAI_API_KEY: your-openai-api-key
# ANTHROPIC_API_KEY: your-anthropic-api-key  
# GOOGLE_API_KEY: your-google-api-key

# Optional: Custom API endpoints
# OPENAI_BASE_URL: https://api.openai.com/v1  # For OpenAI-compatible APIs
# OLLAMA_BASE_URL: http://localhost:11434     # For local Ollama instance

# Example configurations for different providers:

# OpenAI Configuration:
# LLM_PROVIDER: openai
# LLM_MODEL_NAME: gpt-4o-mini          # or gpt-3.5-turbo, gpt-4, etc.
# OPENAI_API_KEY: sk-...

# Anthropic Configuration:
# LLM_PROVIDER: anthropic
# LLM_MODEL_NAME: claude-3-haiku-20240307  # or claude-3-sonnet-20240229, etc.
# ANTHROPIC_API_KEY: sk-ant-...

# Google Gemini Configuration:
# LLM_PROVIDER: google
# LLM_MODEL_NAME: gemini-pro           # or gemini-pro-vision
# GOOGLE_API_KEY: AIza...

# Local Ollama Configuration (no API key needed):
# LLM_PROVIDER: ollama
# LLM_MODEL_NAME: llama2               # or codellama, mistral, etc.
# OLLAMA_BASE_URL: http://localhost:11434
